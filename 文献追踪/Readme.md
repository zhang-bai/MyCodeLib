

è®°å½•ä¸€äº›å‰æ²¿æ–‡çŒ®



[Graph Augmentation](#Graph-Augmentation)

+ [Data Augmentation for Graph Neural Networks   AAAI 2021](#Data-Augmentation-for-Graph-Neural-Networks---AAAI-2021)
+ [Graph-Revised Convolutional Network   arXive 2020](#Graph-Revised-Convolutional-Network)
+ [NodeAug: Semi-Supervised Node Classification with Data Augmentation   KDD2020](#NodeAug-Semi-Supervised-Node-Classification-with-Data-Augmentation)



[Graph Generation](#Graph-Generation)

- [Identifying critical edges in complex networks     Scientific Rrports 2018](#Identifying-critical-edges-in-complex-networks)



[Graph Equivariant](#Graph-Equivariant)

- [Natural Graph Networks    NIPS 2020](#Natural-Graph-Networks)
- [Invariant and Equivariant Graph Networks    ICLR 2019](#Invariant-and-Equivariant-Graph-Networks)
- [E(n) Equivariant Graph Neural Networks   arXive 2021](#E(n)-Equivariant-Graph-Neural-Networks)



[Graph Attack](#Graph Attack)

- [Towards More Practical Adversarial Attacks on Graph Neural Networks    NIPS 2020](#Towards-More-Practical-Adversarial-Attacks-on-Graph-Neural-Networks)



[Recommender Systems](#Recommender-Systems)

- [Factorization Machines    2010 IEEE ICDM](#Factorization-Machines)
- [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction    IJCAI2017](#DeepFM)





# Graph Augmentation



###  [Data Augmentation for Graph Neural Networks   AAAI 2021](https://arxiv.org/abs/2010.04740)

[åŸæ–‡](papers/Data-Augmentation-for-Graph-Neural-Networks.pdf)

[ä»£ç é“¾æ¥](https://github.com/zhao-tong/GAug)

å›¾å¢å¼ºï¼Œä¸»è¦ç ”ç©¶å¢å‡è¾¹å¯¹å›¾æ•°æ®çš„å½±å“

Specifically, we discuss how facilitating message passing by removing â€œnoisyâ€ edges and adding â€œmissingâ€ edges that could exist in the original graph can benefit GNN performance, and its relation to intra-class and inter-class edges.

![](./img/image-20201222162105995.png)

![image-20201222163149287](./img/image-20201222163149287.png)



æ•ˆæœï¼š

![image-20201222163311604](./img/image-20201222163311604.png)





### [Graph-Revised Convolutional Network](https://arxiv.org/abs/1911.07123)



[åŸæ–‡](papers/Graph-Revised-Convolutional-Network.pdf)

[ä»£ç ](https://github.com/Maysir/GRCN)

ä½¿ç”¨ä¸€å€‹gcnä½œçˆ²åœ–ä¿®æ­£æ¨¡å¿«ï¼Œä¸€å€‹gcnä½œçˆ²åœ–åˆ†é¡æ¨¡å¡Šï¼Œ

å°æ–¼adjé æ¸¬ï¼Œåœ¨å¯†é›†å›¾ä¸Šè¿›è¡Œäº†Knearest-neighbourï¼ˆKNNï¼‰ç¨€ç–åŒ–å¤„ç†ï¼šå¯¹äºæ¯ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬å°†è¾¹ç¼˜ä¿ç•™ä¸ºtop-Ké¢„æµ‹åˆ†æ•°ã€‚  KNNç¨€ç–å›¾çš„é‚»æ¥çŸ©é˜µï¼Œè¡¨ç¤ºä¸ºSï¼ˆKï¼‰ï¼Œ

![image-20210206135153225](img/image-20210206135153225.png)



![image-20201224232011361](./img/image-20201224232011361.png)



![image-20210206135248939](img/image-20210206135248939.png)



### [NodeAug Semi-Supervised Node Classification with Data Augmentation](https://bhooi.github.io/papers/nodeaug_kdd20.pdf)

[åŸæ–‡](papers/NodeAug.pdf)

æš‚æ— ä»£ç 



[æ•´ä½“ç»“æ„](#æ•´ä½“ç»“æ„)

[ä¸»è¦æœºåˆ¶](#ä¸»è¦æœºåˆ¶)

[å¯¹æ¯”å®éªŒ](#å¯¹æ¯”æ•ˆæœ)



#### **æ•´ä½“ç»“æ„**

![image-20210206143102301](img/image-20210206143102301.png)



ä»¥èŠ‚ç‚¹ä¸ºä¸­å¿ƒåˆ’åˆ†ä¸ºä¸‰å±‚ï¼ˆ2-hopï¼‰ï¼Œå­é‡Œå‘å¤–åˆ†åˆ«ä¸ºlevel 1 ~ level 3,çº§åˆ«ç”±é«˜åˆ°ä½ï¼Œå³ level 1 higerer than level 2

![image-20210217091745109](img/image-20210217091745109.png)





#### **ä¸»è¦æœºåˆ¶**

- [replacing attributes](#replacing-attributes)
- [removing edges](#removing-edges)
- [adding edges](#adding-edges)
- [subgraph mini-batch training](#subgraph-mini-batch-training)

![image-20210206134946562](img/image-20210206134946562.png)

##### **replacing attributes**



##### **removing edges**

å»è¾¹æ—¶ï¼Œç›®çš„æ˜¯ä¿ç•™é‡è¦çš„è¾¹ï¼Œå»é™¤ä¸é‡è¦çš„è¾¹

å…·æœ‰æ›´å¤§**åº¦**çš„èŠ‚ç‚¹å€¾å‘äºæ›´å…·å½±å“åŠ›ã€‚ä¾‹å¦‚ï¼Œç¤¾äº¤ç½‘ç»œä¸­çš„åäººå¾€å¾€æœ‰å¾ˆå¤šè¿½éšè€…ã€‚

Suppose the degree of the node on its lower end is $ğ‘‘_{ğ‘™ğ‘œğ‘¤}$. We define the score of an edge as:
$$
s_e = log(d_{low})     \qquad (7) 
$$


Suppose the maximum and average edge scores on level ğ‘™ are $ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’max}$ and $ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’avg}$ respectively.

The probability of removing the edge with score ğ‘ ğ‘’ on level ğ‘™ to:
$$
ğ‘_{ğ‘’âˆ’ğ‘Ÿğ‘’ğ‘š} = min \left(
 
ğ‘ğ‘™
\frac{ğ‘ ^{(ğ‘™ )}
_{ğ‘’âˆ’max} âˆ’ ğ‘ _ğ‘’}
{ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’max} - ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’avg}}
, 1

\right )

\qquad (8)
$$
![image-20210217134626722](img/image-20210217134626722.png)





##### adding edges

åœ¨**ä¸­å¿ƒèŠ‚ç‚¹**ä¸**2çº§å’Œ3çº§çš„æŸäº›èŠ‚ç‚¹**ä¹‹é—´æ·»åŠ è¾¹ï¼Œä¾‹å¦‚ï¼Œåœ¨å¼•æ–‡ç½‘ç»œä¸­ï¼Œè®ºæ–‡ A å¼•ç”¨ Bï¼Œå› ä¸ºå®ƒä½¿ç”¨ B ä¸­å¼•å…¥çš„æ–¹æ³• Mã€‚ç„¶è€Œï¼ŒMä¸æ˜¯Bçš„ä¸»è¦è´¡çŒ®ï¼ŒBå¼•ç”¨çš„è®ºæ–‡Cæå‡ºäº†Mã€‚ ç„¶åï¼Œåœ¨ A å’Œ C ä¹‹é—´æ·»åŠ è¾¹ï¼Œåœ¨ä¸æ›´æ”¹å…¶æ ‡ç­¾çš„æƒ…å†µä¸‹å¢å¼º A çš„è¾“å…¥è¦ç´ ã€‚
$$
ğ‘_{ğ‘’âˆ’add} = min \left(
 
\frac{ğ‘}{ğ‘™}
\frac{
ğ‘ _n - ğ‘ ^{(ğ‘™ )}_{nâˆ’min} }
{ğ‘  ^{(ğ‘™)} _{nâˆ’ave} - ğ‘  ^{(ğ‘™)} _{nâˆ’min}}
, 1

\right )

\qquad (8)
$$




ä¸å›¾åƒä¸­å¢å¹¿æ–¹æ³•å¯¹æ¯”ï¼š

removing â€”â€” cutting

adding â€”â€” shearing and resizing, æ”¹å˜å·ç§¯é¡ºåº











##### subgraph mini-batch training





#### å®éªŒç»“æœ



![image-20210206135412484](img/image-20210206135412484.png)









# Graph Generation



### è¡¥å……çŸ¥è¯†

- Graph center / Jordan Center





- Eigenvector centrality

In [graph theory](https://en.wikipedia.org/wiki/Graph_theory), **eigenvector centrality** (also called **eigencentrality** or **prestige score[[1\]](https://en.wikipedia.org/wiki/Eigenvector_centrality#cite_note-:0-1)**) is a measure of the influence of a [node](https://en.wikipedia.org/wiki/Node_(networking)) in a [network](https://en.wikipedia.org/wiki/Network_(mathematics)). Relative scores are assigned to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. A high eigenvector score means that a node is connected to many nodes who themselves have high scores.[[2\]](https://en.wikipedia.org/wiki/Eigenvector_centrality#cite_note-2) [[3\]](https://en.wikipedia.org/wiki/Eigenvector_centrality#cite_note-3)

 The eigenvector centrality thesis reads:

> A node is important if it is linked to by other important nodes.

- **Math**

  Let $A = (a_{i,j})$ be the adjacency matrix of a graph. The eigenvector centrality $x_{i}$ of node $i$ is given by: $$x_i = \frac{1}{\lambda} \sum_k a_{k,i} \, x_k$$ where $\lambda \neq 0$ is a constant. In matrix form we have: $$\lambda x = x A$$

  

  Hence the centrality vector $x$ is the **left-hand eigenvector** of the adjacency matrix $A$ associated with the eigenvalue $\lambda$. It is wise to **choose $\lambda$ as the largest eigenvalue in absolute value of matrix $A$.** By virtue of Perron-Frobenius theorem, this choice guarantees the following desirable property: if matrix $A$ is irreducible, or equivalently if the graph is (strongly) connected, then the eigenvector solution $x$ is both unique and positive.

  

  The **power method** can be used to solve the eigenvector centrality problem. Let $m(v)$ denote the signed component of maximal magnitude of vector $v$. If there is more than one maximal component, let $m(v)$ be the first one. For instance, $m(-3,3,2) = -3$. Let $x^{(0)}$ be an arbitrary vector. For $k \geq 1$:

  1. repeatedly compute $x^{(k)} = x^{(k-1)} A$;
  2. normalize $x^{(k)} = x^{(k)} / m(x^{(k)})$;

  until the desired precision is achieved. It follows that $x^{(k)}$ converges to the dominant eigenvector of $A$ and $m(x^{(k)})$ converges to the dominant eigenvalue of $A$. If matrix $A$ is sparse, each vector-matrix product can be performed in linear time in the size of the graph.

  The method converges when the dominant (largest) and the sub-dominant (second largest) eigenvalues of $A$, respectively denoted by $\lambda_1$ and $\lambda_2$, are separated, that is they are different in absolute value, hence when $|\lambda_1| > |\lambda_2|$. The rate of convergence is the rate at which $(\lambda_2 / \lambda_1)^k$ goes to $0$. Hence, if the sub-dominant eigenvalue is small compared to the dominant one, then the method quickly converges.

  **xå‘é‡å³æ‰€æœ‰Nodeçš„å¤§å°ï¼Œè¯¥å‘é‡å€¼ä»£è¡¨ä¸ªnodeçš„å¾—åˆ†ï¼Œå¹¶æ ¹æ®æœ€å¤§çš„å¾—åˆ†å½’ä¸€åŒ–**

- **Code**

  The built-in function evcent ([R](http://igraph.org/r/doc/evcent.html), [C](http://igraph.org/c/doc/igraph-Structural.html#igraph_eigenvector_centrality)) computes eigenvector centrality.

  A user-defined function eigenvector.centrality follows:

  ```
  # Eigenvector centrality (direct method)
  #INPUT
  # g = graph
  # t = precision
  # OUTPUT
  # A list with:
  # vector = centrality vector
  # value = eigenvalue
  # iter = number of iterations
  
  eigenvector.centrality = function(g, t) {
    A = get.adjacency(g);
    n = vcount(g);
    x0 = rep(0, n);
    x1 = rep(1/n, n);
    eps = 1/10^t;
    iter = 0;
    while (sum(abs(x0 - x1)) > eps) {
      x0 = x1;
      x1 = as.vector(x1 %*% A);
      m = x1[which.max(abs(x1))];
      x1 = x1 / m;
      iter = iter + 1;
    } 
    return(list(vector = x1, value = m, iter = iter))
  }  
  ```







### **Identifying critical edges in complex networks**

[åŸæ–‡](papers/Identifying-critical-edges-in-complex-networks.pdf)

#### æ‘˜è¦

![image-20210304104902929](img/image-20210304104902929.png)





# Graph Equivariant



### Natural Graph Networks

[åŸæ–‡](papers/Natural-Graph-Networks.pdf)

ä»£ç æš‚æ— 





### Invariant and Equivariant Graph Networks

[åŸæ–‡](papers/Invariant-and-equivariant-graph.pdf)



### **E(n) Equivariant Graph Neural Networks**

[åŸæ–‡](papers/E(n)-Equivariant-Graph-Neural-Networks.pdf)



#### æ‘˜è¦

![image-20210304104501862](img/image-20210304104501862.png)







# Graph Attack



### èƒŒæ™¯çŸ¥è¯†

- ##### **å¯¹æŠ—æ”»å‡»**ï¼ˆé’ˆå¯¹èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ï¼‰

  æ”»å‡»åˆ†ç±»ï¼š

  - **Happen Time**

    During model Training   â€”â€”   **poisoning**(ä¸­æ¯’)

    During model Testing     â€”â€”   **evasion**(é€ƒé¿)

  - **Aim**

    Mislead the prediction on specific nodes â€”â€”    **targeted attack**

    Damage the overall task performance 	 â€”â€”    **untargeted attack**

  - **Attackerâ€™s knowledge aobut the model**

    **white-box attacks** â€”â€” full informationï¼ˆmodel parameters, input data, labelsï¼‰

    **grey-box attacks**â€”â€” partial informationï¼ˆthe exact setups vary in a rangeï¼‰

    **black-box**â€”â€” input data and sometimes the black-box predictions of the model

    

- ##### **Inductive Bias (å½’çº³åç½®)**

  > æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹æŸç§å‡è®¾ï¼ˆhypothesisï¼‰çš„åå¥½ï¼Œç§°ä¸ºâ€œå½’çº³åå¥½â€ï¼ˆinductive biasï¼‰ï¼Œæˆ–ç®€ç§°ä¸ºâ€œåå¥½â€

  ä¾‹å¦‚å¯¹ä¸€ç»„æ•°æ®è¿›è¡Œæ‹Ÿåˆçš„æ›²çº¿æœ‰æ— æ•°ç§ï¼Œå…¶ä¸­æœ‰çš„æ¯”è¾ƒâ€œç®€å•â€ï¼ˆå‡è®¾æˆ‘ä»¬è®¤ä¸ºæ›²çº¿æ›´å¹³æ»‘æ„å‘³ç€â€œæ›´ç®€å•â€ï¼‰ï¼Œæœ‰çš„æ›´å¤æ‚ã€‚ä¾‹å¦‚ä¸€ç»„å¯ä»¥ç”¨äºŒæ¬¡æ›²çº¿æ¥æ‹Ÿåˆçš„æ•°æ®ç‚¹ï¼Œç”¨æ›´å¤æ‚çš„æ›´é«˜é˜¶çš„æ›²çº¿ä¹Ÿå¯ä»¥æ‹Ÿåˆï¼Œé‚£æˆ‘ä»¬çš„æ¨¡å‹åº”è¯¥é€‰æ‹©å“ªæ¡æ›²çº¿/å‡è®¾å‘¢ï¼Ÿè¿™å°±æ˜¯æ¨¡å‹å¯¹å‡è®¾çš„åå¥½é—®é¢˜ã€‚

  > æ‰€è°“çš„inductive biasï¼ŒæŒ‡çš„æ˜¯äººç±»å¯¹ä¸–ç•Œçš„**å…ˆéªŒçŸ¥è¯†**ï¼Œå¯¹åº”åœ¨ç½‘ç»œä¸­å°±æ˜¯**ç½‘ç»œç»“æ„**ã€‚

  å½’çº³åå·®æœ‰ç‚¹åƒæˆ‘ä»¬æ‰€è¯´çš„å…ˆéªŒï¼ˆPriorï¼‰ï¼Œä½†æ˜¯æœ‰ç‚¹ä¸åŒçš„æ˜¯å½’çº³åå·®åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ä¸ä¼šæ›´æ–°ï¼Œä½†æ˜¯å…ˆéªŒåœ¨å­¦ä¹ åä¼šä¸æ–­åœ°è¢«æ›´æ–°ã€‚





### Towards More Practical Adversarial Attacks on Graph Neural Networks

[åŸæ–‡](papers/Towards-More-Practical-Adversarial-Attacks-on-Graph-Neural-Networks.pdf)

[ä»£ç ](https://github.com/Mark12Ding/GNN-Practical-Attack)

#### **æ‘˜è¦**

![image-20210220202349627](img/image-20210220202349627.png)

#### æ•´ä½“ç»“æ„

**GC-RWCS** (Greedily Corrected RWCS) strategy

![image-20210220202702811](img/image-20210220202702811.png)



#### ä¸»è¦æœºåˆ¶

- [local constraint on node access](#local-constraint-on-node-access)
- 

åˆ©ç”¨ä¸‹è¯•ä»£æ›¿lossçš„è¯¥å˜é‡ï¼Œç”±ç™½ç›’å˜ä¸ºé»‘ç›’ï¼Œä½¿ä¸åŒ…å«y label
$$
\tilde \delta ^i =C\sum _{j=1} ^N (M^L )_{ji}
$$
å°æ‰°åŠ¨ï¼Œä½¿ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€è¿‘ä¼¼





##### local constraint on node access









#### å®éªŒç»“æœ

![image-20210220203129010](img/image-20210220203129010.png)



**å®éªŒè®¾ç½®ï¼š**

![image-20210220203223115](img/image-20210220203223115.png)











# Recommender Systems



### Factorization Machines







### DeepFM