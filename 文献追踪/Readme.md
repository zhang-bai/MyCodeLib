---
title: å¯¼å¼•
mathjax: True
---



è®°å½•ä¸€äº›å‰æ²¿æ–‡çŒ®,

***è¡¨ç¤ºè¿˜æœªçœ‹



# ç›®å½•



[Graph Survey  ***](#Graph-Survey)

- [Artificial Intelligence Index Report 2021       Stanford University HAI](#Artificial-Intelligence-Index-Report-2021)

- [Graph Self-Supervised Learning: A Survey   arXive 2021  ](#Graph-Self-Supervised-Learning:-A-Survey)



[Graph Augmentation](#Graph-Augmentation)

+ [Data Augmentation for Graph Neural Networks   AAAI 2021](#Data-Augmentation-for-Graph-Neural-Networks---AAAI-2021)
+ [Graph-Revised Convolutional Network   arXive 2020](#Graph-Revised-Convolutional-Network)
+ [NodeAug: Semi-Supervised Node Classification with Data Augmentation   KDD2020](#NodeAug-Semi-Supervised-Node-Classification-with-Data-Augmentation)



[Graph Structures / Generation***](#Graph-Generation)

- [Identifying critical edges in complex networks            Scientific Rrports 2018](#Identifying-critical-edges-in-complex-networks)
- [Struc2vec: Learning Node Representations from Structural Identity     KDD 2017](#Struc2vec:-Learning-Node-Representations-from-Structural-Identity)
- [Representation Learning on Graphs: Methods and Applications        IEEE TCDE 2017](#Representation-Learning-on-Graphs:-Methods-and-Applications)
- [Link Prediction Based on Graph Neural Networks     NIPS 2018](#Link-Prediction-Based-on-Graph-Neural-Networks)
- [Graph similarity scoring and matching      Applied Mathematics Letters 2007](#Graph similarity scoring and matching)
- [Whose Opinion to Follow in Multihypothesis Social Learning? A Large Deviations Perspective     IEEE JSTSP 2015](#Whose-Opinion-to-Follow-in-Multihypothesis-Social-Learning?-A-Large-Deviations-Perspective)



[Graph Equivariant ***](#Graph-Equivariant)

- [Natural Graph Networks    NIPS 2020](#Natural-Graph-Networks)
- [Invariant and Equivariant Graph Networks    ICLR 2019](#Invariant-and-Equivariant-Graph-Networks)
- [E(n) Equivariant Graph Neural Networks   arXive 2021 ***](#E(n)-Equivariant-Graph-Neural-Networks)



[Graph Attack](#Graph Attack)

- [Towards More Practical Adversarial Attacks on Graph Neural Networks    NIPS 2020](#Towards-More-Practical-Adversarial-Attacks-on-Graph-Neural-Networks)



[Recommender Systems](#Recommender-Systems)

- [Factorization Machines      IEEE ICDM 2010](#Factorization-Machines)
- [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction    IJCAI 2017](#DeepFM)





# Graph Survey



### Artificial Intelligence Index Report 2021



### Graph Self-Supervised Learning: A Survey

#### æ‘˜è¦

![image-20210316115421041](img/image-20210316115421041.png)















# Graph Augmentation



###  [Data Augmentation for Graph Neural Networks   AAAI 2021](https://arxiv.org/abs/2010.04740)

[åŸæ–‡](papers/Data-Augmentation-for-Graph-Neural-Networks.pdf)

[ä»£ç é“¾æ¥](https://github.com/zhao-tong/GAug)





#### æ‘˜è¦

![image-20210226183847516](img/image-20210226183847516.png)



#### ç½‘ç»œç»“æ„







å›¾å¢å¼ºï¼Œä¸»è¦ç ”ç©¶å¢å‡è¾¹å¯¹å›¾æ•°æ®çš„å½±å“

Specifically, we discuss how facilitating message passing by removing â€œnoisyâ€ edges and adding â€œmissingâ€ edges that could exist in the original graph can benefit GNN performance, and its relation to intra-class and inter-class edges.

![](./img/image-20201222162105995.png)

![image-20201222163149287](./img/image-20201222163149287.png)



æ•ˆæœï¼š

![image-20201222163311604](./img/image-20201222163311604.png)





### [Graph-Revised Convolutional Network](https://arxiv.org/abs/1911.07123)



[åŸæ–‡](papers/Graph-Revised-Convolutional-Network.pdf)

[ä»£ç ](https://github.com/Maysir/GRCN)

ä½¿ç”¨ä¸€å€‹gcnä½œçˆ²åœ–ä¿®æ­£æ¨¡å¿«ï¼Œä¸€å€‹gcnä½œçˆ²åœ–åˆ†é¡æ¨¡å¡Šï¼Œ

å°æ–¼adjé æ¸¬ï¼Œåœ¨å¯†é›†å›¾ä¸Šè¿›è¡Œäº†Knearest-neighbourï¼ˆKNNï¼‰ç¨€ç–åŒ–å¤„ç†ï¼šå¯¹äºæ¯ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬å°†è¾¹ç¼˜ä¿ç•™ä¸ºtop-Ké¢„æµ‹åˆ†æ•°ã€‚  KNNç¨€ç–å›¾çš„é‚»æ¥çŸ©é˜µï¼Œè¡¨ç¤ºä¸ºSï¼ˆKï¼‰ï¼Œ

![image-20210206135153225](img/image-20210206135153225.png)



![image-20201224232011361](./img/image-20201224232011361.png)



![image-20210206135248939](img/image-20210206135248939.png)



### [NodeAug Semi-Supervised Node Classification with Data Augmentation](https://bhooi.github.io/papers/nodeaug_kdd20.pdf)

[åŸæ–‡](papers/NodeAug.pdf)

æš‚æ— ä»£ç 



[æ•´ä½“ç»“æ„](#æ•´ä½“ç»“æ„)

[ä¸»è¦æœºåˆ¶](#ä¸»è¦æœºåˆ¶)

[å¯¹æ¯”å®éªŒ](#å¯¹æ¯”æ•ˆæœ)



#### **æ•´ä½“ç»“æ„**

![image-20210206143102301](img/image-20210206143102301.png)



ä»¥èŠ‚ç‚¹ä¸ºä¸­å¿ƒåˆ’åˆ†ä¸ºä¸‰å±‚ï¼ˆ2-hopï¼‰ï¼Œå­é‡Œå‘å¤–åˆ†åˆ«ä¸ºlevel 1 ~ level 3,çº§åˆ«ç”±é«˜åˆ°ä½ï¼Œå³ level 1 higerer than level 2

![image-20210217091745109](img/image-20210217091745109.png)





#### **ä¸»è¦æœºåˆ¶**

- [replacing attributes](#replacing-attributes)
- [removing edges](#removing-edges)
- [adding edges](#adding-edges)
- [subgraph mini-batch training](#subgraph-mini-batch-training)

![image-20210206134946562](img/image-20210206134946562.png)

##### **replacing attributes**



##### **removing edges**

å»è¾¹æ—¶ï¼Œç›®çš„æ˜¯ä¿ç•™é‡è¦çš„è¾¹ï¼Œå»é™¤ä¸é‡è¦çš„è¾¹

å…·æœ‰æ›´å¤§**åº¦**çš„èŠ‚ç‚¹å€¾å‘äºæ›´å…·å½±å“åŠ›ã€‚ä¾‹å¦‚ï¼Œç¤¾äº¤ç½‘ç»œä¸­çš„åäººå¾€å¾€æœ‰å¾ˆå¤šè¿½éšè€…ã€‚

Suppose the degree of the node on its lower end is $ğ‘‘_{ğ‘™ğ‘œğ‘¤}$. We define the score of an edge as:
$$
s_e = log(d_{low})     \qquad (7) 
$$


Suppose the maximum and average edge scores on level ğ‘™ are $ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’max}$ and $ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’avg}$ respectively.

The probability of removing the edge with score ğ‘ ğ‘’ on level ğ‘™ to:
$$
ğ‘_{ğ‘’âˆ’ğ‘Ÿğ‘’ğ‘š} = min \left(
 
ğ‘ğ‘™
\frac{ğ‘ ^{(ğ‘™ )}
_{ğ‘’âˆ’max} âˆ’ ğ‘ _ğ‘’}
{ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’max} - ğ‘  ^{(ğ‘™)} _{ğ‘’âˆ’avg}}
, 1

\right )

\qquad (8)
$$
![image-20210217134626722](img/image-20210217134626722.png)





##### adding edges

åœ¨**ä¸­å¿ƒèŠ‚ç‚¹**ä¸**2çº§å’Œ3çº§çš„æŸäº›èŠ‚ç‚¹**ä¹‹é—´æ·»åŠ è¾¹ï¼Œä¾‹å¦‚ï¼Œåœ¨å¼•æ–‡ç½‘ç»œä¸­ï¼Œè®ºæ–‡ A å¼•ç”¨ Bï¼Œå› ä¸ºå®ƒä½¿ç”¨ B ä¸­å¼•å…¥çš„æ–¹æ³• Mã€‚ç„¶è€Œï¼ŒMä¸æ˜¯Bçš„ä¸»è¦è´¡çŒ®ï¼ŒBå¼•ç”¨çš„è®ºæ–‡Cæå‡ºäº†Mã€‚ ç„¶åï¼Œåœ¨ A å’Œ C ä¹‹é—´æ·»åŠ è¾¹ï¼Œåœ¨ä¸æ›´æ”¹å…¶æ ‡ç­¾çš„æƒ…å†µä¸‹å¢å¼º A çš„è¾“å…¥è¦ç´ ã€‚
$$
ğ‘_{ğ‘’âˆ’add} = min \left(
 
\frac{ğ‘}{ğ‘™}
\frac{
ğ‘ _n - ğ‘ ^{(ğ‘™ )}_{nâˆ’min} }
{ğ‘  ^{(ğ‘™)} _{nâˆ’ave} - ğ‘  ^{(ğ‘™)} _{nâˆ’min}}
, 1

\right )

\qquad (8)
$$




ä¸å›¾åƒä¸­å¢å¹¿æ–¹æ³•å¯¹æ¯”ï¼š

removing â€”â€” cutting

adding â€”â€” shearing and resizing, æ”¹å˜å·ç§¯é¡ºåº











##### subgraph mini-batch training





#### å®éªŒç»“æœ



![image-20210206135412484](img/image-20210206135412484.png)









# Graph Generation



### è¡¥å……çŸ¥è¯†

##### 1.Graph center / Jordan Center

The **center** (or [Jordan](https://en.wikipedia.org/wiki/Camille_Jordan) center[[1\]](https://en.wikipedia.org/wiki/Graph_center#cite_note-WF-1)) of a [graph](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) is the set of all vertices of minimum [eccentricity](https://en.wikipedia.org/wiki/Eccentricity_(graph_theory)),[[2\]](https://en.wikipedia.org/wiki/Graph_center#cite_note-2) that is, the set of all vertices *u* where the greatest distance *d*(*u*,*v*) to other vertices *v* is minimal. Equivalently, it is the set of vertices with eccentricity equal to the graph's [radius](https://en.wikipedia.org/wiki/Radius_(graph_theory)).[[3\]](https://en.wikipedia.org/wiki/Graph_center#cite_note-3) Thus vertices in the center (**central points**) minimize the maximal distance from other points in the graph.

This is also known as the **vertex 1-center problem** and can be extended to the [vertex k-center problem](https://en.wikipedia.org/wiki/Vertex_k-center_problem).

Finding the center of a graph is useful in [facility location problems](https://en.wikipedia.org/wiki/Facility_location_problem) where the goal is to minimize the worst-case distance to the facility. For example, placing a hospital at a central point reduces the longest distance the ambulance has to travel.

The center can be found using the [Floydâ€“Warshall algorithm](https://en.wikipedia.org/wiki/Floydâ€“Warshall_algorithm).[[4\]](https://en.wikipedia.org/wiki/Graph_center#cite_note-4)[[5\]](https://en.wikipedia.org/wiki/Graph_center#cite_note-5) Another algorithm has been proposed based on matrix calculus.[[6\]](https://en.wikipedia.org/wiki/Graph_center#cite_note-P-6)

The concept of the center of a graph is related to the [closeness centrality](https://en.wikipedia.org/wiki/Closeness_centrality) measure in [social network analysis](https://en.wikipedia.org/wiki/Social_network_analysis), which is the reciprocal of the mean of the distances *d*(*A*,*B*).[[1\]](https://en.wikipedia.org/wiki/Graph_center#cite_note-WF-1)





##### 2.Eigenvector centrality

In [graph theory](https://en.wikipedia.org/wiki/Graph_theory), **eigenvector centrality** (also called **eigencentrality** or **prestige score[[1\]](https://en.wikipedia.org/wiki/Eigenvector_centrality#cite_note-:0-1)**) is a measure of the influence of a [node](https://en.wikipedia.org/wiki/Node_(networking)) in a [network](https://en.wikipedia.org/wiki/Network_(mathematics)). Relative scores are assigned to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. A high eigenvector score means that a node is connected to many nodes who themselves have high scores.[[2\]](https://en.wikipedia.org/wiki/Eigenvector_centrality#cite_note-2) [[3\]](https://en.wikipedia.org/wiki/Eigenvector_centrality#cite_note-3)

 The eigenvector centrality thesis reads:

> A node is important if it is linked to by other important nodes.

- **Math**

  Let $A = (a_{i,j})$ be the adjacency matrix of a graph. The eigenvector centrality $x_{i}$ of node $i$ is given by: $$x_i = \frac{1}{\lambda} \sum_k a_{k,i} \, x_k$$ where $\lambda \neq 0$ is a constant. In matrix form we have: $$\lambda x = x A$$

  

  Hence the centrality vector $x$ is the **left-hand eigenvector** of the adjacency matrix $A$ associated with the eigenvalue $\lambda$. It is wise to **choose $\lambda$ as the largest eigenvalue in absolute value of matrix $A$.** By virtue of Perron-Frobenius theorem, this choice guarantees the following desirable property: if matrix $A$ is irreducible, or equivalently if the graph is (strongly) connected, then the eigenvector solution $x$ is both unique and positive.

  

  The **power method** can be used to solve the eigenvector centrality problem. Let $m(v)$ denote the signed component of maximal magnitude of vector $v$. If there is more than one maximal component, let $m(v)$ be the first one. For instance, $m(-3,3,2) = -3$. Let $x^{(0)}$ be an arbitrary vector. For $k \geq 1$:

  1. repeatedly compute $x^{(k)} = x^{(k-1)} A$;
  2. normalize $x^{(k)} = x^{(k)} / m(x^{(k)})$;

  until the desired precision is achieved. It follows that $x^{(k)}$ converges to the dominant eigenvector of $A$ and $m(x^{(k)})$ converges to the dominant eigenvalue of $A$. If matrix $A$ is sparse, each vector-matrix product can be performed in linear time in the size of the graph.

  The method converges when the dominant (largest) and the sub-dominant (second largest) eigenvalues of $A$, respectively denoted by $\lambda_1$ and $\lambda_2$, are separated, that is they are different in absolute value, hence when $|\lambda_1| > |\lambda_2|$. The rate of convergence is the rate at which $(\lambda_2 / \lambda_1)^k$ goes to $0$. Hence, if the sub-dominant eigenvalue is small compared to the dominant one, then the method quickly converges.

  **xå‘é‡å³æ‰€æœ‰Nodeçš„å¤§å°ï¼Œè¯¥å‘é‡å€¼ä»£è¡¨ä¸ªnodeçš„å¾—åˆ†ï¼Œå¹¶æ ¹æ®æœ€å¤§çš„å¾—åˆ†å½’ä¸€åŒ–**

- **Code**

  The built-in function evcent ([R](http://igraph.org/r/doc/evcent.html), [C](http://igraph.org/c/doc/igraph-Structural.html#igraph_eigenvector_centrality)) computes eigenvector centrality.

  A user-defined function eigenvector.centrality follows:

  ```
  # Eigenvector centrality (direct method)
  #INPUT
  # g = graph
  # t = precision
  # OUTPUT
  # A list with:
  # vector = centrality vector
  # value = eigenvalue
  # iter = number of iterations
  
  eigenvector.centrality = function(g, t) {
    A = get.adjacency(g);
    n = vcount(g);
    x0 = rep(0, n);
    x1 = rep(1/n, n);
    eps = 1/10^t;
    iter = 0;
    while (sum(abs(x0 - x1)) > eps) {
      x0 = x1;
      x1 = as.vector(x1 %*% A);
      m = x1[which.max(abs(x1))];
      x1 = x1 / m;
      iter = iter + 1;
    } 
    return(list(vector = x1, value = m, iter = iter))
  }  
  ```



##### 3. Clique

In the [mathematical](https://en.wikipedia.org/wiki/Mathematics) area of [graph theory](https://en.wikipedia.org/wiki/Graph_theory), a **clique** ([/ËˆkliËk/](https://en.wikipedia.org/wiki/Help:IPA/English) or [/ËˆklÉªk/](https://en.wikipedia.org/wiki/Help:IPA/English)) is a subset of vertices of an [undirected graph](https://en.wikipedia.org/wiki/Undirected_graph) such that every two distinct vertices in the clique are adjacent. That is, a clique of a graph *G* is an [induced subgraph](https://en.wikipedia.org/wiki/Induced_subgraph) of *G* that is [complete](https://en.wikipedia.org/wiki/Complete_graph). Cliques are one of the basic concepts of graph theory and are used in many other mathematical problems and constructions on graphs. Cliques have also been studied in [computer science](https://en.wikipedia.org/wiki/Computer_science): the task of finding whether there is a clique of a given size in a [graph](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) (the [clique problem](https://en.wikipedia.org/wiki/Clique_problem)) is [NP-complete](https://en.wikipedia.org/wiki/NP-complete), but despite this hardness result, many algorithms for finding cliques have been studied.

Although the study of [complete subgraphs](https://en.wikipedia.org/wiki/Complete_graph) goes back at least to the graph-theoretic reformulation of [Ramsey theory](https://en.wikipedia.org/wiki/Ramsey_theory) by [ErdÅ‘s & Szekeres (1935)](https://en.wikipedia.org/wiki/Clique_(graph_theory)#CITEREFErdÅ‘sSzekeres1935),[[1\]](https://en.wikipedia.org/wiki/Clique_(graph_theory)#cite_note-1) the term *clique* comes from [Luce & Perry (1949)](https://en.wikipedia.org/wiki/Clique_(graph_theory)#CITEREFLucePerry1949), who used complete subgraphs in [social networks](https://en.wikipedia.org/wiki/Social_network) to model [cliques](https://en.wikipedia.org/wiki/Clique) of people; that is, groups of people all of whom know each other. Cliques have many other applications in the sciences and particularly in [bioinformatics](https://en.wikipedia.org/wiki/Bioinformatics).









## **Identifying critical edges in complex networks**

[åŸæ–‡](papers/Identifying-critical-edges-in-complex-networks.pdf)

#### æ‘˜è¦

![image-20210304104902929](img/image-20210304104902929.png)

æå‡ºäº†ä¸€ç§åŸºäºå›¢ç°‡å’Œç½‘ç»œè·¯å¾„çš„è¾¹ç¼˜æ’åºç®—æ³•$BCC_{MOD} (Betweenness Centrality and Clique Model)$ã€‚

performs good in identifying critical edges both in network connectivity and spreading dynamic



#### æ•´ä½“ç»“æ„

- ç›®çš„

  æ‰¾å‡ºç½‘ç»œè¿æ¥å’Œä¼ æ’­è¿‡ç¨‹ä¸­çš„å…³é”®è¾¹

- æ–¹æ³•

  åŸºäºç½‘ç»œä¸­çš„å›¢å’Œè·¯å¾„æå‡ºBCCæ–¹æ³•ï¼Œ
  $$
  BCC_{MOD}(u,v) = \frac{k_u k_v \cdot BC(u,v)}{\sum_{i=3}^n C(u,v)_i}
  $$
  å°†èŠ‚ç‚¹çš„åº¦ã€Betweenness centralityã€å›¢çš„æ¦‚å¿µç»“åˆèµ·æ¥ï¼Œæ ¸å¿ƒæ€æƒ³å³ï¼š

  - è¿æ¥ä¸¤ä¸ªåº¦é«˜çš„èŠ‚ç‚¹çš„è¾¹å¾ˆé‡è¦
  - é€šè¿‡è¯¥è¾¹çš„æœ€çŸ­è·¯å¾„è¶Šå¤šï¼Œè¯¥è¾¹è¶Šé‡è¦
  - å›¢(å®Œå…¨å­å›¾)çš„èŠ‚ç‚¹æ•°å¤šçš„å­å›¾ä¸­ï¼Œåˆ é™¤è¾¹è¿˜æœ‰å¾ˆå¤šå…¶ä»–æ–¹å¼åˆ°è¾¾ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œå½±å“è¾ƒå°



#### Related Work

**è¡¡é‡ Node çš„é‡è¦æ€§ï¼š**

- Node's degree

  Degree centrality, semi-local centrality, k-shell and H-index

- Paths in networks

  Closeness centrality, betweenness centrality and eccentricity centrality

- Eigenvector

  PageRank, LeaderRank and HITs



**è¡¡é‡ Edge çš„é‡è¦æ€§ï¼š**

- **Degree product** 

  supposes that edges connecting two nodes with high degrees are critical.

- **Betweenness centrality** of edges and betweenness centrality of a group of edges

  suppose that edges linking two connected components are important.

- **Jaccard coefficient**, if node i and node j have a lot of common neighbors, even if they have no direct connection, information also can spread from node i to node j easily,
  so edges are more important if there are less common neighbors.

  Jaccard index  , åˆç§°ä¸ºJaccardç›¸ä¼¼ç³»æ•°ï¼ˆJaccard similarity coefficientï¼‰ç”¨äºæ¯”è¾ƒæœ‰é™æ ·æœ¬é›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ä¸å·®å¼‚æ€§ã€‚Jaccardç³»æ•°å€¼è¶Šå¤§ï¼Œæ ·æœ¬ç›¸ä¼¼åº¦è¶Šé«˜ã€‚

- **Edges in the clique** 

  if an edge is removed, information can spread through other edges in clique  which contains the removed edge 

  so, intuitively, edges in smaller cliques are more important.

- The ability to disseminate information

  An edge is important if most of the information is spreading through this edge



**æœ¬æ–‡ï¼š**

â€‹	In this report, we only use the topology of networks to rank the importance of edges, considering not only
the local characteristics (degrees of nodes, cliques) but also the global characteristics (betweenness centrality).





#### ä¸»è¦æœºåˆ¶

##### 1. Betweenness centrality

The more the shortest paths between node pairs pass through the edge e(u, v), the more important the edge e(u, v) is. The betweenness centrality of an edge e(u, v)15 is defined as:
$$
BC(u,v) = \sum_{s \ne t \in V} \frac{\delta_{st}(u,v)}{\delta_{st}}
$$


where $\delta_{st}$ is the number of all the shortest paths between node s and node t, $\delta_{st}(u, v)$  is the number of all the
shortest paths between node s and node t which pass through the edge e(u, v), the larger the score BC is, the more
important the edge is.



##### 2. Edges in the clique

The more important the two related nodes are, the more important the edge is. **On the other hand**, if there are many different cliques containing e(u, v), even e(u, v) is removed, the information also can spread from u to v (or v to u) easily through other edges in these cliques.



##### 3. $BCC_{MOD}$ (Betweenness Centrality and Clique Model)


$$
BCC_{MOD}(u,v) = \frac{k_u k_v \cdot BC(u,v)}{\sum_{i=3}^n C(u,v)_i}
$$
Where BC(u, v) is the betweenness centrality of edge e(u, v), $k_u$ and $k_v$  are the degrees of node u and node v respectively, $C(u, v)_i$ is the number of cliques containing edge e(u, v) (in this report, clique means full connected subgraph, not the maximum full connected subgraph) **whose size being i**. For example $C(u, v)_4 = 3$ means there are three cliques containing edge e(u, v) whose size being 4.**(i å³å®Œå…¨å­å›¾ä¸­çš„èŠ‚ç‚¹æ•°)**

In this method, the larger the score is, the more important the edge is.

![image-20210309204332132](img/image-20210309204332132.png) 

For example, as shown in Fig. 5(a,c), the degrees of nodes 1 and 2 are 7 and 8 respectively. In Fig. 5(a) (max size of cliques is 4), $C(1, 2)_3$ is 5 and $C(1, 2)_4$ is 2.(æ ¹æ®æ¯ä¸ªèŠ‚ç‚¹è¿å‡ºæ¥å¤šå°‘ä¸ªè¾¹ç¡®å®š)

When we remove edge e(1, 2), there are also many paths from node 1 to node 2, the effect of spreading is little.
However, in Fig. 5(c) (max size of cliques is 3) with C(1, 2)3 being 1, when we remove edge e(1, 2), the effect of
spreading is large since there is only one path (1, 3, 2) from node 1 to node 2. Table 4 shows the effect probability
$p_e$ of nodes 2, 3, and 9 with the original infected source being node 1 on SIR spreading model with full contact
process. Taking node 2 as an example, in Fig. 5(a,b), its effect probability is 0.3733 and 0.2240 respectively under
Î¼ = 0.2. However, in Fig. 5(c,d), the effect probability of node 2 is 0.2392 and 0.0380 respectively under Î¼ = 0.2.

![image-20210310084814775](img/image-20210310084814775.png)



The Jaccard coefficient of an edge e(u, v) is defined asï¼š
$$
J_{e(u,v)} = \frac{|\Gamma _ u \cap \Gamma_v |}{|\Gamma _ u \cup \Gamma_v |}
$$
where u and v are two related nodes of the edge e(u, v) and $\Gamma _ u$ is the set of $uâ€™s$  neighbors. 

Jaccardç›¸ä¼¼ç³»æ•°ï¼ˆJaccard similarity coefficientï¼‰ç”¨äºæ¯”è¾ƒæœ‰é™æ ·æœ¬é›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ä¸å·®å¼‚æ€§ã€‚Jaccardç³»æ•°å€¼è¶Šå¤§ï¼Œæ ·æœ¬ç›¸ä¼¼åº¦è¶Šé«˜ã€‚

The Bridgeness index of an edge e(u, v) is defined as

![image-20210310090804969](img/image-20210310090804969.png)



#### å®éªŒç»“æœ



- æ•°æ®é›†

![image-20210309163212661](img/image-20210309163212661-1615278754326.png)

- è¯„ä»·æŒ‡æ ‡

  - Susceptibility index S
    $$
    S = \sum _{s<s_{max}} \frac{n_s s^2}{n}
    $$

  ![image-20210309164044789](img/image-20210309164044789.png)

  - The size of giant component Ïƒ

  

  

  - SIR spreading model



## Struc2vec: Learning Node Representations from Structural Identity







## Representation Learning on Graphs: Methods and Applications





## Link Prediction Based on Graph Neural Networks



## Graph similarity scoring and matching



#### æ‘˜è¦

![image-20210310092923360](img/image-20210310092923360.png)





## Whose Opinion to Follow in Multihypothesis Social Learning? A Large Deviations Perspective



# Graph Equivariant



## Natural Graph Networks

[åŸæ–‡](papers/Natural-Graph-Networks.pdf)

ä»£ç æš‚æ— 





## Invariant and Equivariant Graph Networks

[åŸæ–‡](papers/Invariant-and-equivariant-graph.pdf)



## **E(n) Equivariant Graph Neural Networks**

[åŸæ–‡](papers/E(n)-Equivariant-Graph-Neural-Networks.pdf)



#### æ‘˜è¦

![image-20210304104501862](img/image-20210304104501862.png)







# Graph Attack



### èƒŒæ™¯çŸ¥è¯†

- ##### **å¯¹æŠ—æ”»å‡»**ï¼ˆé’ˆå¯¹èŠ‚ç‚¹åˆ†ç±»é—®é¢˜ï¼‰

  æ”»å‡»åˆ†ç±»ï¼š

  - **Happen Time**

    During model Training   â€”â€”   **poisoning**(ä¸­æ¯’)

    During model Testing     â€”â€”   **evasion**(é€ƒé¿)

  - **Aim**

    Mislead the prediction on specific nodes â€”â€”    **targeted attack**

    Damage the overall task performance 	 â€”â€”    **untargeted attack**

  - **Attackerâ€™s knowledge aobut the model**

    **white-box attacks** â€”â€” full informationï¼ˆmodel parameters, input data, labelsï¼‰

    **grey-box attacks**â€”â€” partial informationï¼ˆthe exact setups vary in a rangeï¼‰

    **black-box**â€”â€” input data and sometimes the black-box predictions of the model

    

- ##### **Inductive Bias (å½’çº³åç½®)**

  > æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹æŸç§å‡è®¾ï¼ˆhypothesisï¼‰çš„åå¥½ï¼Œç§°ä¸ºâ€œå½’çº³åå¥½â€ï¼ˆinductive biasï¼‰ï¼Œæˆ–ç®€ç§°ä¸ºâ€œåå¥½â€

  ä¾‹å¦‚å¯¹ä¸€ç»„æ•°æ®è¿›è¡Œæ‹Ÿåˆçš„æ›²çº¿æœ‰æ— æ•°ç§ï¼Œå…¶ä¸­æœ‰çš„æ¯”è¾ƒâ€œç®€å•â€ï¼ˆå‡è®¾æˆ‘ä»¬è®¤ä¸ºæ›²çº¿æ›´å¹³æ»‘æ„å‘³ç€â€œæ›´ç®€å•â€ï¼‰ï¼Œæœ‰çš„æ›´å¤æ‚ã€‚ä¾‹å¦‚ä¸€ç»„å¯ä»¥ç”¨äºŒæ¬¡æ›²çº¿æ¥æ‹Ÿåˆçš„æ•°æ®ç‚¹ï¼Œç”¨æ›´å¤æ‚çš„æ›´é«˜é˜¶çš„æ›²çº¿ä¹Ÿå¯ä»¥æ‹Ÿåˆï¼Œé‚£æˆ‘ä»¬çš„æ¨¡å‹åº”è¯¥é€‰æ‹©å“ªæ¡æ›²çº¿/å‡è®¾å‘¢ï¼Ÿè¿™å°±æ˜¯æ¨¡å‹å¯¹å‡è®¾çš„åå¥½é—®é¢˜ã€‚

  > æ‰€è°“çš„inductive biasï¼ŒæŒ‡çš„æ˜¯äººç±»å¯¹ä¸–ç•Œçš„**å…ˆéªŒçŸ¥è¯†**ï¼Œå¯¹åº”åœ¨ç½‘ç»œä¸­å°±æ˜¯**ç½‘ç»œç»“æ„**ã€‚

  å½’çº³åå·®æœ‰ç‚¹åƒæˆ‘ä»¬æ‰€è¯´çš„å…ˆéªŒï¼ˆPriorï¼‰ï¼Œä½†æ˜¯æœ‰ç‚¹ä¸åŒçš„æ˜¯å½’çº³åå·®åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ä¸ä¼šæ›´æ–°ï¼Œä½†æ˜¯å…ˆéªŒåœ¨å­¦ä¹ åä¼šä¸æ–­åœ°è¢«æ›´æ–°ã€‚





## Towards More Practical Adversarial Attacks on Graph Neural Networks

[åŸæ–‡](papers/Towards-More-Practical-Adversarial-Attacks-on-Graph-Neural-Networks.pdf)

[ä»£ç ](https://github.com/Mark12Ding/GNN-Practical-Attack)

#### **æ‘˜è¦**

![image-20210220202349627](img/image-20210220202349627.png)

#### æ•´ä½“ç»“æ„

**GC-RWCS** (Greedily Corrected RWCS) strategy

![image-20210220202702811](img/image-20210220202702811.png)



#### ä¸»è¦æœºåˆ¶

- [local constraint on node access](#local-constraint-on-node-access)
- 

åˆ©ç”¨ä¸‹è¯•ä»£æ›¿lossçš„è¯¥å˜é‡ï¼Œç”±ç™½ç›’å˜ä¸ºé»‘ç›’ï¼Œä½¿ä¸åŒ…å«y label
$$
\tilde \delta ^i =C\sum _{j=1} ^N (M^L )_{ji}
$$
å°æ‰°åŠ¨ï¼Œä½¿ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€è¿‘ä¼¼





##### local constraint on node access









#### å®éªŒç»“æœ

![image-20210220203129010](img/image-20210220203129010.png)



**å®éªŒè®¾ç½®ï¼š**

![image-20210220203223115](img/image-20210220203223115.png)











# Recommender Systems



## Factorization Machines







## DeepFM